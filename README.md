# OCP Install automation with Ansible

## 1. Description

All this code permits you to deploy an OCP cluster on 3DS Outscale automatically with Ansible. You have the possibility to set your parameters to adapt your cluster to your needs (number of nodes, cpu, memory, disk, etc...).

To install OCP in general, you have two methods :
* IPI : Installer Provide Infrastructure
* UPI : User Provide Infrastructure

## 2. Architecture
 
 ![Schema](docs/ocp-3ds-outscale.svg)

## 3. Files organization description

* 01_ocp_packer_image : files to create RHCOS images with Hashicorp Packer
* 02_ocp_create_network : files to create needed VPC and network components in your account automatically with Hashicorp Terraform
* 03_ocp_install : files to create OpenShift Cluster in your Outscale account with end to end automation based on Ansible and Hashicorp Terraform

## 4. What this automation do

* Deploy an infrastructure VM with :
	- Web server based on Nginx to provide Ignition files to OCP RHOCS nodes
	- A DNS server with all needed records for your OCP cluster (api, api-int, *.apps, etc...)
	- A Load Balancer based on HAProxy as endpoint for the OCP cluster with 4 frontends

* Deploy a temporary OCP bootstrap node to install your cluster. This node will be deleted at the end of this automation.

* Deploy 3 OCP master nodes as requested by the user.

* Deploy x OCP worker nodes as requested by the user.

* Deploy GitOps (ArgoCD) on your OCP cluster if you requested it in vars file

To give you an estimate, a cluster with 6 worker nodes takes around 10 - 30 min to be up and running from scratch with this automation (depending your Internet connection)

**Note :** Today, this automation permit only to have master and worker nodes with dynamic IPs provided by 3DS Outscale (DHCP).

## 5. Prerequisites

- [ ] Your environment have Internet access
- [ ] A 3DS Outscale space
- [ ] Red Hat account with valid subscription (RHEL, AAP, OCP)
	- RHEL8 BaseOS repo : rhel-8-for-x86_64-baseos-rpms
	- RHEL8 AppStream repo : rhel-8-for-x86_64-appstream-rpms
	- AAP2 repo : ansible-automation-platform-2.1-for-rhel-8-x86_64-rpms
- [ ] Ansible Execution Environment : 
	- quay.io/david_martini/ocp_nuta:4.9 to deploy OCP4.9
	- quay.io/david_martini/ocp_nuta:4.10 to deploy OCP4.10
- [ ] One VM bastion used to deploy automation with :
	- RHEL8
	- podman
	- ansible-navigator
	- Ansible Execution Environment with all tools needed by this automation scripts
- [ ] A defined IP for your infravm (ex: 192.168.40.10). This server have a major role during deployment (DNS server & LB) and this IP is very important. This IP can be outside of the DHCP scope to haven't clash IP.
	

## 6. Deployment

All steps must be done on your RHEL8 VM Bastion or your RHEL8 workstation connected to the Internet (to pull image) with direct access to your Nutanix cluster. 

✅ **Please read the entire documentation step by step to deploy without error**

**1. Register your RHEL8 VM Bastion**
```
subscription-manager register
```

**2. Attach a subsciption to your RHEL8 VM Bastion**
```
subscription-manager attach --pool=<your pool ID>
```

**3. Enable needed repos on RHEL8 VM Bastion**
```
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms \
--enable=rhel-8-for-x86_64-appstream-rpms \
--enable=ansible-automation-platform-2.1-for-rhel-8-x86_64-rpms
```

**4. Install tools and utilities on RHEL8 VM Bastion**
```
dnf install git vim bind-utils jq podman ansible-navigator
```

**5. Pull Ansible EE with all embedded tools on your RHEL8 VM Bastion**
```
podman pull quay.io/david_martini/ocp_nuta:4.x
```

**6. Clone GIT repository on your RHEL8 VM Bastion**
```
git clone https://github.com/davmartini/rh_nutanix_fr.git
```

**7. Go to GIT workspace and edit vars file with your configuration**
```
cd $PATH/rh_nutanix_fr/ocp_install/
vim ansible/vars/vars.yaml
```

**8. Custom DNS on VM Bastion**

All the automation is embedded in a container and your don't need to install extra tools directly on your bastion. Howerver, this container or Execution Environment need to resolv OCP cluster records to install properly it.

The container or Execution Environment inherite of DNS configuration you have on your Bastion. Before to start the deployment you must so add the infravm IP as **first** DNS server as follow :

```
# Generated by NetworkManager
search nutarh.io
nameserver ${infravmip}
nameserver 10.42.32.10
nameserver 10.42.32.11
```


**10. Deploy your cluster with Ansible**
> :heavy_exclamation_mark: Before to deploy your cluster, be sure your have configured infravm IP as first DNS server in IPAM configuration of your AOS subnet.
```
ansible-navigator run ansible/main.yml -i ansible/inventory --eei quay.io/david_martini/ocp_nuta:4.x -m stdout --pae false --lf /tmp/ansible-navigator.log
```

**11. Cluster information**

A the end of automatic deployment, all needed information to connet to your cluster will be printed (kubeconfig, URL, kubeadmin-passowrd). 
> :heavy_exclamation_mark: Be sure to save these informations. If you lose them, you couldn't connect to your cluster and you will have no possibility to recover them

## 7. Access to your custer from Mac OSX
You can access to you fresh installed cluster from your remote computer to define send all requests to infravm EIP.

* Install dnsmasq
```
brew install dnsmasq
```

* Show dnsmasq version
```
brew info dnsmasq
```

* Modify dnsmasq config file
```
vim /usr/local/etc/dnsmasq.conf
conf-file=/Users/dmartini/.dnsmasq/dnsmasq.conf
```

* Create new config file
```
vim /Users/dmartini/.dnsmasq/dnsmasq.conf
# *.ocpoutscale.local wildcard will be resolved as 127.0.0.1, including subdomains where 171.33.93.77 is infravm EIP
address=/*.ocpoutscale.local/171.33.93.77 
listen-address=127.0.0.1
```

* Restart dnsmasq
```
sudo brew services restart dnsmasq
```

* Add 127.0.0.1 as your first dns server in your Mac OSX network configuration


## 8. More information

* You can check needed variables with **vars.yaml.example** example.
* You have access to HAProxy stats for each frontend on the URL : http://${Infra VM IP}:5000/stats. You will need to set a stats password in the vars file.
* To access your cluster from a remote computer, you have to add infravm IP as DNS server in your own network configuration.

## 9. Day 2 operation

* Enable a container registry for your OpenShift Cluster: [Registry configuration](https://docs.openshift.com/container-platform/4.10/installing/installing_platform_agnostic/installing-platform-agnostic.html#installation-registry-storage-config_installing-platform-agnostic)
* Configure persistent storage for Monitoring stack: [Monitoring configuration](https://docs.openshift.com/container-platform/4.10/monitoring/configuring-the-monitoring-stack.html#configuring-persistent-storage)
* Enable authentification provider: [Auth configuration](https://docs.openshift.com/container-platform/4.10/authentication/understanding-authentication.html)
* Migrate Load Balancer frontends and DNS records on your production equipments
